@startuml Vision Shepherd System

' Abstract base classes and interfaces
package "Models" {
    abstract class BaseModel {
        - device: str
        - model: Any
        + __init__(model_path: str, device: str)
        + load_model()
        + preprocess(image: np.ndarray): torch.Tensor
        + postprocess(output: Any): Dict
    }

    abstract class DetectionModel {
        - confidence_threshold: float
        - nms_threshold: float
        + detect(image: np.ndarray): List[Detection]
    }

    abstract class SegmentationModel {
        + segment(image: np.ndarray): List[Mask]
    }

    abstract class CaptioningModel {
        + generate_caption(image: np.ndarray): str
    }

    abstract class DepthModel {
        + estimate_depth(image: np.ndarray): np.ndarray
    }

    abstract class EmbeddingModel {
        + encode_image(image: np.ndarray): np.ndarray
        + encode_text(text: str): np.ndarray
    }


    ' Concrete implementations
    class YOLO {
        - confidence_threshold: float
        - nms_threshold: float
        + detect(image: np.ndarray): List[Detection]
    }

    class SAM {
        - points_per_side: int
        - pred_iou_thresh: float
        + segment(image: np.ndarray): List[Mask]
        - _generate_points(): np.ndarray
    }

    class CLIP {
        - text_encoder: TextEncoder
        - image_encoder: ImageEncoder
        + encode_image(image: np.ndarray): Vector
        + encode_text(text: str): Vector
    }


    class BLIP {
        - image_captioner: ImageCaptioner
        + caption_image(image: np.ndarray): Vector
    }

    class DAN {
        + estimate_depth(image: np.ndarray): np.ndarray
    }
}

' DatabaseWrapper component
class DatabaseWrapper {
    + store_object(embedding: np.ndarray, position: np.ndarray, metadata: Dict): bool
    + retrieve_objects(query: Dict): List[Dict]
    + check_object_exists(position: np.ndarray, embedding: np.ndarray): Tuple[bool, str]
}



' Main Shepherd class
class Shepherd {
    - detector: DetectionModel
    - segmenter: SegmentationModel
    - captioner: CaptioningModel
    - depth_estimator: Optional[DepthModel]
    - DatabaseWrapper: DatabaseWrapper
    - device: str
    - config: Configuration
    
    + __init__(config: Dict, DatabaseWrapper: DatabaseWrapper)
    + process_frame(image: np.ndarray, position: np.ndarray
    + update_config(config: Dict)
    + get_object_map(): Dict
    - _validate_models()
    - _process_detections(image: np.ndarray): List[np.ndarray]
    - _process_segments(image: np.ndarray, detections: List[np.ndarray]): List[Mask]
    - _process_captions(image: np.ndarray, detections: List[np.ndarray]): List[str]
    - _estimate_depths(image: np.ndarray): Optional[np.ndarray]
}
' Configuration
class Configuration {
    + model_paths: Dict[str, str]
    + thresholds: Dict[str, float]
    + device: str
    + resolution: Tuple[int, int]
}

' Relationships
BaseModel <|-- DetectionModel
BaseModel <|-- SegmentationModel
BaseModel <|-- CaptioningModel
BaseModel <|-- DepthModel
BaseModel <|-- EmbeddingModel
DetectionModel <|.. YOLO
SegmentationModel <|.. SAM
DepthModel <|.. DAN
CaptioningModel <|.. BLIP
EmbeddingModel <|.. CLIP


Shepherd o-- DetectionModel
Shepherd o-- SegmentationModel
Shepherd o-- CaptioningModel
Shepherd o-- DepthModel
Shepherd o-- EmbeddingModel
Shepherd o-- DatabaseWrapper

Configuration o-- Shepherd

@enduml
